{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d3f30bf",
   "metadata": {},
   "source": [
    "## Analyze Common Crawl Data with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee8d3ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ba7d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['367855\\t172-in-addr\\tarpa\\t1',\n",
       " '367856\\taddr\\tarpa\\t1',\n",
       " '367857\\tamphic\\tarpa\\t1',\n",
       " '367858\\tbeta\\tarpa\\t1',\n",
       " '367859\\tcallic\\tarpa\\t1',\n",
       " '367860\\tch\\tarpa\\t1',\n",
       " '367861\\td\\tarpa\\t1',\n",
       " '367862\\thome\\tarpa\\t7',\n",
       " '367863\\tiana\\tarpa\\t1',\n",
       " '367907\\tlocal\\tarpa\\t1']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Domains CSV File into an RDD\n",
    "common_crawl_domain_counts = sc.textFile('./crawl/cc-main-limited-domains.csv')\n",
    "\n",
    "# Display first few domains from the RDD\n",
    "common_crawl_domain_counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7950d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_domain_graph_entry(entry):\n",
    "    \"\"\"\n",
    "    Formats a Common Crawl domain graph entry. Extracts the site_id, \n",
    "    top-level domain (tld), domain name, and subdomain count as seperate items.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the entry on delimiter ('\\t') into site_id, domain, tld, and num_subdomains\n",
    "    site_id, domain, tld, num_subdomains = entry.split('\\t')        \n",
    "    return int(site_id), domain, tld, int(num_subdomains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02f9ed9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(367855, '172-in-addr', 'arpa', 1),\n",
       " (367856, 'addr', 'arpa', 1),\n",
       " (367857, 'amphic', 'arpa', 1),\n",
       " (367858, 'beta', 'arpa', 1),\n",
       " (367859, 'callic', 'arpa', 1),\n",
       " (367860, 'ch', 'arpa', 1),\n",
       " (367861, 'd', 'arpa', 1),\n",
       " (367862, 'home', 'arpa', 7),\n",
       " (367863, 'iana', 'arpa', 1),\n",
       " (367907, 'local', 'arpa', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply `fmt_domain_graph_entry` to the raw data RDD\n",
    "formatted_host_counts = common_crawl_domain_counts.map(fmt_domain_graph_entry)\n",
    "\n",
    "# Display the first few entries of the new RDD\n",
    "formatted_host_counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb75dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 7, 1, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_subdomain_counts(entry):\n",
    "    \"\"\"\n",
    "    Extract the subdomain count from a Common Crawl domain graph entry.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the entry on delimiter ('\\t') into site_id, domain, tld, and num_subdomains\n",
    "    site_id, domain, tld, num_subdomains = entry.split('\\t')\n",
    "    \n",
    "    # return ONLY the num_subdomains\n",
    "    return int(num_subdomains)\n",
    "\n",
    "\n",
    "# Apply `extract_subdomain_counts` to the raw data RDD\n",
    "host_counts = common_crawl_domain_counts.map(extract_subdomain_counts)\n",
    "\n",
    "# Display the first few entries\n",
    "host_counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa284001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595466"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce the RDD to a single value, the sum of subdomains, with a lambda function\n",
    "# as the reduce function\n",
    "total_host_counts = host_counts.reduce(lambda x,y: x+y )\n",
    "\n",
    "# Display result count\n",
    "total_host_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "562745d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the sparkContext and the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f129687d",
   "metadata": {},
   "source": [
    "## Exploring Domain Counts with PySpark DataFrames and SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99565365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "297084db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+----+---+\n",
      "|     _c0|           _c1| _c2|_c3|\n",
      "+--------+--------------+----+---+\n",
      "|  367855|   172-in-addr|arpa|  1|\n",
      "|  367856|          addr|arpa|  1|\n",
      "|  367857|        amphic|arpa|  1|\n",
      "|  367858|          beta|arpa|  1|\n",
      "|  367859|        callic|arpa|  1|\n",
      "|  367860|            ch|arpa|  1|\n",
      "|  367861|             d|arpa|  1|\n",
      "|  367862|          home|arpa|  7|\n",
      "|  367863|          iana|arpa|  1|\n",
      "|  367907|         local|arpa|  1|\n",
      "|  367908|           nic|arpa|  1|\n",
      "|48987160|     1-20media|coop|  1|\n",
      "|48987161|           134|coop|  1|\n",
      "|48987162|            19|coop|  4|\n",
      "|48987163|   1strochdale|coop|  1|\n",
      "|48987164|          2012|coop|  4|\n",
      "|48987165|2012intlsummit|coop|  1|\n",
      "|48987166|      2147mans|coop|  1|\n",
      "|48987167|   21stcentury|coop|  1|\n",
      "|48987168|   2decologico|coop|  1|\n",
      "+--------+--------------+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the target file into a DataFrame\n",
    "common_crawl = spark.read.options(delimiter='\\t', inferSchema='True').csv(\"./crawl/cc-main-limited-domains.csv\")\n",
    "\n",
    "\n",
    "# Display the DataFrame to the notebook\n",
    "common_crawl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f7b4ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+----------------+--------------+\n",
      "| site_id|        domain|top_level_domain|num_subdomains|\n",
      "+--------+--------------+----------------+--------------+\n",
      "|  367855|   172-in-addr|            arpa|             1|\n",
      "|  367856|          addr|            arpa|             1|\n",
      "|  367857|        amphic|            arpa|             1|\n",
      "|  367858|          beta|            arpa|             1|\n",
      "|  367859|        callic|            arpa|             1|\n",
      "|  367860|            ch|            arpa|             1|\n",
      "|  367861|             d|            arpa|             1|\n",
      "|  367862|          home|            arpa|             7|\n",
      "|  367863|          iana|            arpa|             1|\n",
      "|  367907|         local|            arpa|             1|\n",
      "|  367908|           nic|            arpa|             1|\n",
      "|48987160|     1-20media|            coop|             1|\n",
      "|48987161|           134|            coop|             1|\n",
      "|48987162|            19|            coop|             4|\n",
      "|48987163|   1strochdale|            coop|             1|\n",
      "|48987164|          2012|            coop|             4|\n",
      "|48987165|2012intlsummit|            coop|             1|\n",
      "|48987166|      2147mans|            coop|             1|\n",
      "|48987167|   21stcentury|            coop|             1|\n",
      "|48987168|   2decologico|            coop|             1|\n",
      "+--------+--------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the DataFrame's columns with `withColumnRenamed()`\n",
    "common_crawl = common_crawl.withColumnRenamed(\"_c0\", \"site_id\") \\\n",
    "        .withColumnRenamed(\"_c1\", \"domain\") \\\n",
    "        .withColumnRenamed(\"_c2\", \"top_level_domain\") \\\n",
    "        .withColumnRenamed(\"_c3\", \"num_subdomains\")\n",
    "  \n",
    "# Display the first few rows of the DataFrame and the new schema\n",
    "common_crawl.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff524e08",
   "metadata": {},
   "source": [
    "## Reading and Writing Datasets to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33be3162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the `common_crawl` DataFrame to a series of parquet files\n",
    "common_crawl.write.parquet(\"./results/common_crawl/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a99ceb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+----------------+--------------+\n",
      "| site_id|        domain|top_level_domain|num_subdomains|\n",
      "+--------+--------------+----------------+--------------+\n",
      "|  367855|   172-in-addr|            arpa|             1|\n",
      "|  367856|          addr|            arpa|             1|\n",
      "|  367857|        amphic|            arpa|             1|\n",
      "|  367858|          beta|            arpa|             1|\n",
      "|  367859|        callic|            arpa|             1|\n",
      "|  367860|            ch|            arpa|             1|\n",
      "|  367861|             d|            arpa|             1|\n",
      "|  367862|          home|            arpa|             7|\n",
      "|  367863|          iana|            arpa|             1|\n",
      "|  367907|         local|            arpa|             1|\n",
      "|  367908|           nic|            arpa|             1|\n",
      "|48987160|     1-20media|            coop|             1|\n",
      "|48987161|           134|            coop|             1|\n",
      "|48987162|            19|            coop|             4|\n",
      "|48987163|   1strochdale|            coop|             1|\n",
      "|48987164|          2012|            coop|             4|\n",
      "|48987165|2012intlsummit|            coop|             1|\n",
      "|48987166|      2147mans|            coop|             1|\n",
      "|48987167|   21stcentury|            coop|             1|\n",
      "|48987168|   2decologico|            coop|             1|\n",
      "+--------+--------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- site_id: integer (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- top_level_domain: string (nullable = true)\n",
      " |-- num_subdomains: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from parquet directory\n",
    "common_crawl_domains = spark.read.parquet(\"./results/common_crawl/\")\n",
    "\n",
    "# Display the first few rows of the DataFrame and the schema\n",
    "common_crawl_domains.show()\n",
    "common_crawl_domains.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96f34ede",
   "metadata": {},
   "source": [
    "## Querying Domain Counts with PySpark DataFrames and SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdd04b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view in the metadata for this `SparkSession`\n",
    "common_crawl_domains.createOrReplaceTempView(\"crawl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8f79679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|top_level_domain|sum(num_subdomains)|\n",
      "+----------------+-------------------+\n",
      "|             edu|             484438|\n",
      "|             gov|              85354|\n",
      "|          travel|              10768|\n",
      "|            coop|               8683|\n",
      "|            jobs|               6023|\n",
      "|            post|                143|\n",
      "|             map|                 40|\n",
      "|            arpa|                 17|\n",
      "+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using DataFrame methods\n",
    "common_crawl_domains_2 = common_crawl_domains.groupBy(\"top_level_domain\").sum(\"num_subdomains\").orderBy(\"sum(num_subdomains)\", ascending=False)\n",
    "common_crawl_domains_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dae3e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|top_level_domain|sum(num_subdomains)|\n",
      "+----------------+-------------------+\n",
      "|             edu|             484438|\n",
      "|             gov|              85354|\n",
      "|          travel|              10768|\n",
      "|            coop|               8683|\n",
      "|            jobs|               6023|\n",
      "|            post|                143|\n",
      "|             map|                 40|\n",
      "|            arpa|                 17|\n",
      "+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using SQL\n",
    "query = \"\"\" SELECT top_level_domain,SUM(num_subdomains) FROM crawl GROUP BY top_level_domain ORDER BY SUM(num_subdomains) DESC\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b45051e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+\n",
      "|top_level_domain|num_subdomains|\n",
      "+----------------+--------------+\n",
      "|             gov|           178|\n",
      "+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame using DataFrame Methods\n",
    "common_crawl_domains_3 = common_crawl_domains.filter((common_crawl_domains.domain == \"nps\") & (common_crawl_domains.top_level_domain == \"gov\")).select(\"top_level_domain\", \"num_subdomains\")\n",
    "common_crawl_domains_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1746a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+\n",
      "|top_level_domain|num_subdomains|\n",
      "+----------------+--------------+\n",
      "|             gov|           178|\n",
      "+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame using SQL\n",
    "query = \"\"\" SELECT top_level_domain, num_subdomains\n",
    "FROM crawl\n",
    "WHERE domain = 'nps' AND top_level_domain = 'gov'\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2233037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the notebook's `SparkSession` and `sparkContext`\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
